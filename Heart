# ================================
# IMPORT LIBRARY
# ================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix

# ================================
# SETUP
# ================================
pd.set_option('display.max_columns', None)
np.random.seed(42)

# ================================
# 1. LOAD DATA DAN EDA
# ================================
print("1. Loading and EDA...")

column_names = [
    'age', 'sex', 'cp', 'trestbps', 'chol',
    'fbs', 'restecg', 'thalach', 'exang',
    'oldpeak', 'slope', 'ca', 'thal', 'num'
]

df = pd.read_csv("processed.cleveland.data", names=column_names, na_values="?")

# üëâ Tampilkan 5 baris awal data
print("\nüìå Data Awal (5 baris):")
print(df.head())

print(f"Shape: {df.shape}")
print(df.info())

print("\nMissing Values:")
print(df.isnull().sum())

total_missing = df.isnull().sum().sum()
duplicate_count = df.duplicated().sum()
print(f"\nüîç Total Missing Values: {total_missing}")
print(f"üìë Duplikat Sebelum Dihapus: {duplicate_count}")

# ================================
# 2. VISUALISASI
# ================================
print("\n2. Visualisasi...")

df.hist(figsize=(15, 10))
plt.tight_layout()
plt.savefig("numeric_histograms.png")
plt.close()

plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Matrix")
plt.savefig("correlation_matrix.png")
plt.close()

# ================================
# 3. DATA CLEANING
# ================================
print("\n3. Data Cleaning...")

print("\nüëâ Missing value sebelum diperbaiki:")
print(df.isnull().sum())

for col in df.columns:
    if df[col].dtype == "object":
        df[col] = df[col].fillna(df[col].mode()[0])  # Imputasi mode untuk kolom object
    else:
        df[col] = pd.to_numeric(df[col], errors="coerce")
        df[col] = df[col].fillna(df[col].median())  # Imputasi median untuk numerik

# üëâ Tambahan: tampilkan kembali missing value setelah diperbaiki
print("\n‚úÖ Missing value setelah diperbaiki:")
print(df.isnull().sum())

# üëâ Tambahan: tampilkan data duplikat (jika ada)
print(f"\nüìë Duplikat ditemukan: {df.duplicated().sum()}")
if df.duplicated().sum() > 0:
    print("üëâ Contoh data duplikat:")
    print(df[df.duplicated()].head())

before_drop_duplicates = df.shape[0]
df.drop_duplicates(inplace=True)
after_drop_duplicates = df.shape[0]
removed_duplicates = before_drop_duplicates - after_drop_duplicates

print(f"\nüßπ Jumlah Data Duplikat yang Dihapus: {removed_duplicates}")
print(f"Data shape after cleaning: {df.shape}")
print("\nüìå Data Setelah Cleaning (5 baris):")
print(df.head())

# ================================
# 4. ENCODING & TARGET LABEL
# ================================
print("\n4. Encoding target...")

df["num"] = df["num"].apply(lambda x: 1 if x > 0 else 0)
income_map = {0: "Tidak Sakit", 1: "Sakit"}
encoding_maps = {"num": income_map}

# üëâ Tampilkan 5 baris setelah encoding
print("\nüìå Data Setelah Encoding (5 baris):")
print(df.head())

# ================================
# 5. CORRELATION AFTER CLEANING
# ================================
print("\n5. Correlation after cleaning...")

plt.figure(figsize=(15, 15))
sns.heatmap(df.corr(), annot=True, square=True, cmap="RdYlGn_r")
plt.title("Correlation After Cleaning")
plt.savefig("correlation_after_encoding.png")
plt.close()

# ================================
# 6. FEATURE SELECTION
# ================================
print("\n6. Feature Selection...")

def correlation(dataset, threshold):
    col_corr = set()
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                col_corr.add(corr_matrix.columns[i])
    return col_corr

X = df.drop("num", axis=1)
y = df["num"]

corr_features = correlation(X, 0.8)
X.drop(columns=corr_features, inplace=True)

# üëâ Tampilkan fitur yang tersisa
print("\nüìå Fitur Setelah Seleksi Korelasi:")
print(X.columns.tolist())

# üëâ Tampilkan 5 baris data fitur
print("\nüìå Data Fitur (5 baris):")
print(X.head())

# ================================
# 7. MODEL TRAINING
# ================================
print("\n7. Model Training...")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'criterion': ['gini', 'entropy']
}

grid = GridSearchCV(RandomForestClassifier(random_state=42),
                    param_grid, scoring="accuracy", cv=3, n_jobs=-1, verbose=1)

try:
    grid.fit(X_train, y_train)
    best_model = grid.best_estimator_
    print(f"Best Parameters: {grid.best_params_}")
except Exception as e:
    print("‚ùå GridSearchCV gagal:", str(e))
    exit()

# ================================
# FEATURE IMPORTANCE
# ================================
plt.figure(figsize=(10, 6))
feature_importance = pd.DataFrame({
    "Feature": X.columns,
    "Importance": best_model.feature_importances_
}).sort_values("Importance", ascending=False)

sns.barplot(x="Importance", y="Feature", data=feature_importance)
plt.title("Feature Importance")
plt.tight_layout()
plt.savefig("feature_importance.png")
plt.close()
print("‚úÖ Feature Importance disimpan sebagai 'feature_importance.png'")


# ================================
# FEATURE IMPORTANCE
# ================================
plt.figure(figsize=(10, 6))
feature_importance = pd.DataFrame({
    "Feature": X.columns,
    "Importance": best_model.feature_importances_
}).sort_values("Importance", ascending=False)

sns.barplot(x="Importance", y="Feature", data=feature_importance)
plt.title("Feature Importance")
plt.tight_layout()
plt.savefig("feature_importance.png")
plt.close()
print("‚úÖ Feature Importance disimpan sebagai 'feature_importance.png'")

# ================================
# 9. SAVE MODEL
# ================================
print("\n9. Saving model...")

model_components = {
    "model": best_model,
    "feature_names": X.columns.tolist(),
    "encoding_maps": encoding_maps,
    "model_params": grid.best_params_,
    "removed_features": list(corr_features),
    "target_map": income_map
}
joblib.dump(model_components, "heart_disease_model.joblib")
print("Model saved as 'heart_disease_model.joblib'")

# ================================
# 10. PREDICTION TEST
# ================================
def predict_heart_disease(data, model_components):
    if isinstance(data, dict):
        data = pd.DataFrame([data])
    model = model_components['model']
    features = model_components['feature_names']
    data_for_pred = data[features]
    pred = model.predict(data_for_pred)[0]
    prob = model.predict_proba(data_for_pred)[0]
    return {
        'prediction': pred,
        'prediction_label': model_components['target_map'][pred],
        'probability': prob[pred]
    }

test_sample = X_test.iloc[0].to_dict()
loaded = joblib.load("heart_disease_model.joblib")
print("\nüìå Prediction test result:")
print(predict_heart_disease(test_sample, loaded))
print("Actual:", y_test.iloc[0])

# ================================
# 11. PROBABILITAS
# ================================
probas = best_model.predict_proba(X_test)
print("\nüìå Contoh 5 Probabilitas Prediksi:")
print(probas[:5])

## ================================
# 12. RINGKASAN AKHIR
# ================================
print("\nüìä Ringkasan Akhir Evaluasi:")

# Ulangi prediksi jika belum ada
if 'y_pred' not in locals():
    y_pred = best_model.predict(X_test)

print(f"Total data latih: {len(X_train)}")
print(f"Total data uji  : {len(X_test)}")
print(f"Akurasi Model   : {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision       : {precision_score(y_test, y_pred):.4f}")
print(f"Recall          : {recall_score(y_test, y_pred):.4f}")
print(f"Prediksi benar  : {(y_pred == y_test).sum()} dari {len(y_test)} sampel")


